import init
import WebBaseLoader as WB
import pdfLoader as PDF
import jsonLoader as JL
import pptLoader as PL
import testClass as TC
import csvLoader as CL
import CLIP_RAG as CLIP
if init.platform.system() == "Windows": 
    import hwpLoader as HL
import docxLoader_old as WL
from test_grok import create_image
from trellis import create_3d_from_image
import HWP








# API í‚¤ ì •ë³´ ë¡œë“œ
init.load_dotenv()

#init.logging.langsmith("Pickcare-RAG")

# tqdm ê°ì²´ë¥¼ ì „ì—­ ë³€ìˆ˜ë¡œ ì„ ì–¸ (callbackì—ì„œ ê³µìœ )
progress_bar = None

def callback(step: int, steps: int, time: float):
    print("Completed step: {} of {}".format(step, steps))
    
def callback2(step: int, steps: int, time: float):
    global progress_bar
    if progress_bar is None:
        progress_bar = init.tqdm(total=steps, desc="Generating Image")  # ì´ˆê¸°í™”
    progress_bar.update(1)  # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
    if step == steps:  # ì™„ë£Œ ì‹œ í´ë¦¬ì–´
        progress_bar.close()

def format_docs(docs):
    # ê²€ìƒ‰í•œ ë¬¸ì„œ ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ ë¬¸ë‹¨ìœ¼ë¡œ í•©ì³ì¤ë‹ˆë‹¤.
    return "\n\n".join(doc.page_content for doc in docs)



def WebLoad(url, model, QA, attrs, html_class, prompt = None):
    parseMan = WB.WebBaseLoader(url)
    docs = parseMan.load(attrArgs = attrs, klass = html_class)

    #ë¬¸ì„œë¶„í• 
    text_splitter = init.RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 50)

    splits = text_splitter.split_documents(docs)

    #ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
    vectorstore = init.FAISS.from_documents(documents = splits, embedding = init.OpenAIEmbeddings(model="text-embedding-3-large"))
    
    #ëª¨ë¸ ìƒì„±
    llm = init.ChatOpenAI(model_name = model)

    #ê²€ìƒ‰(search)
    retriever = vectorstore.as_retriever()
    
    if prompt == None:
        prompt = init.hub.pull("rlm/rag-prompt") #í”„ë¡¬í”„íŠ¸
    
    #ì²´ì¸ ìƒì„±
    rag_chain = (
    {"context": retriever | format_docs, "question": init.RunnablePassthrough()}
    | prompt
    | llm
    | init.StrOutputParser()
    )
    #ì²´ì¸ ì‹¤í–‰
    question = QA
    response  = rag_chain.invoke(question)
    
    response_buff = list()
    response_buff.append(f"URL: {url}")
    response_buff.append(f"ë¬¸ì„œì˜ ìˆ˜: {len(docs)}")
    response_buff.append(f"[HUMAN]\n{question}\n")
    response_buff.append(f"[AI]\n{response}")
    return response_buff


def PDFask(file_path, model, QA, prompt = None, k = 3):
    loader = PDF.pdfLoader(file_path= file_path, extract_bool=True)
    
    text_splitter = init.RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 50)
    
    split_docs = loader.load_and_split(text_splitter = text_splitter)
    #print(type(split_docs))
    vectorstore = init.FAISS.from_documents(documents= split_docs, embedding = init.OpenAIEmbeddings(model="text-embedding-3-large"))
    
    
    bm25_retriever = init.BM25Retriever.from_documents(split_docs)    
    bm25_retriever.k = k
    faiss_vectorstore = init.FAISS.from_documents(split_docs, init.OpenAIEmbeddings(model="text-embedding-3-large"))
    faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs = {"k" : k})
    
    
    #ì•™ìƒë¸” ë¦¬íŠ¸ë¦¬ë²„ë¥¼ ì´ˆê¸°í™”
    ensemble_retiever = init.EnsembleRetriever(
        retrievers=[bm25_retriever, faiss_retriever], weight = [0.5, 0.5]
    )
    
    #í”„ë¡¬í”„íŠ¸ ìƒì„±
    if prompt == None:
        prompt = init.hub.pull("rlm/rag-prompt") #í”„ë¡¬í”„íŠ¸
    
    llm = init.ChatOpenAI(model_name = model)
    
    rag_chain = (
        {"context" : ensemble_retiever | format_docs, "question" : init.RunnablePassthrough()}
        |prompt
        |llm
        |init.StrOutputParser()
    )
    
    response = rag_chain.invoke(QA)
    response_buff = list()
    response_buff.append(f"PDF Path: {file_path}")
    response_buff.append(f"[HUMAN]\n{QA}\n")
    response_buff.append(f"[AI]\n{response}")
    return response_buff


def JSONask(file_path, jq_schema,  QA , model = "gpt-5", prompt = None, k = 3, text_content = False):
    loader = JL.jsonLoader(file_path = file_path, jq_schema= jq_schema, text_content = text_content)
    
    text_splitter = init.RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 50)
    loader.load()
    split_docs = loader.load_and_split(text_splitter=text_splitter)
    #vectorstore = init.FAISS.from_documents(documents = split_docs, embedding = init.OpenAIEmbeddings(model = "text-embedding-3-large"))
    
    bm25_retriever = init.BM25Retriever.from_documents(split_docs)    
    bm25_retriever.k = k
    faiss_vectorstore = init.FAISS.from_documents(split_docs, init.OpenAIEmbeddings(model="text-embedding-3-large"))
    faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs = {"k" : k})
    #ì•™ìƒë¸” ë¦¬íŠ¸ë¦¬ë²„ë¥¼ ì´ˆê¸°í™”
    ensemble_retiever = init.EnsembleRetriever(
        retrievers=[bm25_retriever, faiss_retriever], weight = [0.5, 0.5]
    )
    
    if prompt == None:
        prompt = init.hub.pull("rlm/rag-prompt") #í”„ë¡¬í”„íŠ¸
    
    llm = init.ChatOpenAI(model_name = model)
    
    rag_chain = (
        {"context" : ensemble_retiever | format_docs, "question" : init.RunnablePassthrough()}
        |prompt
        |llm
        |init.StrOutputParser()
    )
    
    response = rag_chain.invoke(QA)
    response_buff = list()
    response_buff.append(f"JSON Path: {file_path}")
    response_buff.append(f"[HUMAN]\n{QA}\n")
    response_buff.append(f"[AI]\n{response}")
    return response_buff
        
        
def HWPask(file_path, QA, model = "gpt-5", prompt = None, k = 3):
    loader = HL.hwpLoader(file_path= file_path)
    context = loader.load2()
    text_splitter = init.RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 50)
    text_chunks = text_splitter.split_text(context)
    # ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ë¥¼ Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    split_docs = [init.Document(page_content=chunk, metadata={"source": file_path}) for chunk in text_chunks]
    #vectorstore = init.FAISS.from_documents(documents = split_docs, embedding = init.OpenAIEmbeddings(model = "text-embedding-3-large"))
    
    bm25_retriever = init.BM25Retriever.from_documents(split_docs)    
    bm25_retriever.k = k
    faiss_vectorstore = init.FAISS.from_documents(split_docs, init.OpenAIEmbeddings(model="text-embedding-3-large"))
    faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs = {"k" : k})
    #ì•™ìƒë¸” ë¦¬íŠ¸ë¦¬ë²„ë¥¼ ì´ˆê¸°í™”
    ensemble_retiever = init.EnsembleRetriever(
        retrievers=[bm25_retriever, faiss_retriever], weight = [0.5, 0.5]
    )
    
    if prompt == None:
        prompt = init.hub.pull("rlm/rag-prompt") #í”„ë¡¬í”„íŠ¸
    
    prompt += "\n answer in korean"
    
    
    llm = init.ChatOpenAI(model_name = model)
    
    rag_chain = (
        {"context" : ensemble_retiever | format_docs, "question" : init.RunnablePassthrough()}
        #{"context" : init.RunnableLambda(format_docs) | ensemble_retiever, "question" : init.RunnablePassthrough()}
        |prompt
        |llm
        |init.StrOutputParser()
    )
    
    response = rag_chain.invoke(QA)
    response_buff = list()
    response_buff.append(f"HWP Path: {file_path}")
    response_buff.append(f"[HUMAN]\n{QA}\n")
    response_buff.append(f"[AI]\n{response}")
    return response_buff
    
    
    


def promptMaker(Prompt : dict):
    system_prompt = Prompt['system']
    question = Prompt['question']
    prompt = init.ChatPromptTemplate.from_messages(
        [
            ("system",
             f"{system_prompt}"
            ),
            init.MessagesPlaceholder(variable_name= "chat_history"),
            ("human" , "#Question:\n{question}"),  # ì‚¬ìš©ì ì…ë ¥ì„ ë³€ìˆ˜ë¡œ ì‚¬ìš©
        ]
        
    )
    return prompt


# ì„¸ì…˜ IDë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì„¸ì…˜ ê¸°ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
def get_session_history(session_ids):
    print(f"[ëŒ€í™” ì„¸ì…˜ID]: {session_ids}")
    if session_ids not in store:  # ì„¸ì…˜ IDê°€ storeì— ì—†ëŠ” ê²½ìš°
        # ìƒˆë¡œìš´ ChatMessageHistory ê°ì²´ë¥¼ ìƒì„±í•˜ì—¬ storeì— ì €ì¥
        store[session_ids] = init.ChatMessageHistory()
    return store[session_ids]  # í•´ë‹¹ ì„¸ì…˜ IDì— ëŒ€í•œ ì„¸ì…˜ ê¸°ë¡ ë°˜í™˜
    

def simpleChatWithHistory(ask):
    prompt = promptMaker(Prompt = ask)
    llm = init.ChatOpenAI()
    chain = prompt | llm | init.StrOutputParser()
    #ì„¸ì…˜ ê¸°ë¡ì„ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬
    
    chain_with_history = init.RunnableWithMessageHistory(
        chain, 
        get_session_history, #ì„¸ì…˜ ê¸°ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
        input_messages_key = "question", #ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ í…œí”Œë¦¿ ë³€ìˆ˜ì— ë“¤ì–´ê°ˆ key
        history_messages_key= "chat_history", #ê¸°ë¡ ë©”ì‹œì§€ì˜ í‚¤
    )
    
    
    response = chain_with_history.invoke(
        #ì§ˆë¬¸ ì…ë ¥
        {"question" : ask["question"]},
        config={"configurable": {"session_id": "abc123"}},
         
    )
    return response


def RAG_RunnableWithMessageHistory(file_path, ask : dict,  session_id: dict , model = "gpt-5-mini",temp = 0,  k=3, chunk_size = 1000, chunk_overlap = 50):
    #ë¬¸ì„œ ë¡œë“œ
    loader = PDF.pdfLoader(file_path=file_path, extract_bool= True)
    docs = loader.load()
    #ë¬¸ì„œ ë¶„í• 
    text_splitter = init.RecursiveCharacterTextSplitter(chunk_size = chunk_size, chunk_overlap = chunk_overlap)
    split_documents = text_splitter.split_documents(docs)
    #ì„ë² ë”© ìƒì„±
    embeddings = init.OpenAIEmbeddings()
    
    #DBìƒì„± ë° ì €ì¥
    vectorstore = init.FAISS.from_documents(documents= split_documents, embedding= embeddings)
    
    #ê²€ìƒ‰ê¸° (Retriever) ìƒì„±
    retriever = vectorstore.as_retriever(search_kwargs = {"k" : k})
    
    #í”„ë¡¬í”„íŠ¸ ìƒì„±

    
    system = ask['system']
    prompt = init.PromptTemplate.from_template(
            """
                You are an assistant for question-answering tasks. 
                Use the following pieces of retrieved context to answer the question. 
                If you don't know the answer, just say that you don't know. 
                Answer in Korean.


            #Previous Chat History:
            {chat_history}

            #Question: 
            {question} 

            #Context: 
            {context} 

            #Answer:"""
    )
    
    
    if "gpt-5" in model:
    #ëª¨ë¸ ìƒì„±
        llm = init.ChatOpenAI(model_name = model, temperature= None)
    else:
        llm = init.ChatOpenAI(model_name = model, temperature= temp)
    
    
    #ì²´ì¸ ìƒì„±
    chain = (
        {
            "context" : init.itemgetter("question") | retriever,
            "question" : init.itemgetter("question"),
            "chat_history" : init.itemgetter("chat_history"),
        }
        | prompt
        | llm
        | init.StrOutputParser()
    )
    
    global store
    store = dict()
    
    
    rag_with_history = init.RunnableWithMessageHistory(
        chain, 
        get_session_history, #ì„¸ì…˜ ê¸°ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
        input_messages_key= "question", #ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ í…œí”Œë¦¿ ë³€ìˆ˜ì— ë“¤ì–´ê°ˆ key
        history_messages_key= "chat_history",  #ê¸°ë¡ ë©”ì‹œì§€ì˜ í‚¤
        
    )
    
    response = rag_with_history.invoke(
        #ì§ˆë¬¸ ì…ë ¥
        {"question" : ask["question"]},
        config = {"configurable" : {"session_id" : session_id["session_id"]}},
    )
    
    return response

def prompt_maker():
    custom_prompt = init.PromptTemplate(
    input_variables=["context", "question"],
    template="ëƒ¥! ì €ëŠ” ë¬¸ì„œë¥¼ ì½ê³  ë§í•  ì¤„ ì•„ëŠ” ë˜‘ë˜‘í•œ ê³ ì–‘ì´ì˜ˆìš”~ ğŸ˜º\n{context}ë¥¼ ë³´ê³ , {question}ì— ëŒ€í•´ ìµœëŒ€í•œ ê·€ì—½ê³  ì‚¬ë‘ìŠ¤ëŸ½ê³  ì¹´ì™€ì´í•œ ê³ ì–‘ì´ ë§íˆ¬ë¡œ ì •ë¦¬í•´ì¤„ê²Œìš”! ì•¼ì˜¹~ ë‹µë³€ì€ ì•„ì£¼ ë””í…Œì¼í•˜ê³  ë‚´ ì„¬ì„¸í•œ ìˆ˜ì—¼ì²˜ëŸ¼ ì´ˆ~ ì„¼ì„œí‹°ë¸Œí•˜ê²Œ ë‹µë³€í•´ì¤„ê²Œ ëƒ¥ëƒ¥. ë‹µë³€ì´ ë§Œì¡±ìŠ¤ëŸ¬ìš°ë©´ ê³ ê¸‰ ì¸„ë¥´ í•œ ê°œ ì¤„ë˜ëƒ¥?. \në‹µë³€: ",
    )
    return custom_prompt
    
def create_diffusion_image(file_name, prompt):
    if init.platform.system() == "Windows": 
        stable_diffusion = init.StableDiffusion(
            model_path=r"D:\Coding\PythonCleanCode\practice_20250710\model\prefectPonyXL_v40.safetensors",
            # wtype="default", # Weight type (e.g. "q8_0", "f16", etc) (The "default" setting is automatically applied and determines the weight type of a model file)
        )
    elif init.platform.system() == "Linux":
        stable_diffusion = init.StableDiffusion(
            model_path=r"/mnt/d/Coding/PythonCleanCode/practice_20250710/model/prefectPonyXL_v40.safetensors",
            # wtype="default", # Weight type (e.g. "q8_0", "f16", etc) (The "default" setting is automatically applied and determines the weight type of a model file)
        )
        
    output = stable_diffusion.txt_to_img(
        prompt=prompt,
        width=512, # Must be a multiple of 64
        height=512, # Must be a multiple of 64
        progress_callback=callback2,
        # seed=1337, # Uncomment to set a specific seed (use -1 for a random seed)
    )
    output[0].save(f"output_images/{file_name}.png") # Output returned as list of PIL Images
    
    
    image_path = f"output_images/{file_name}.png"
    if init.os.path.exists(image_path):
        img = init.Image.open(image_path)
        
        init.plt.rc('font', family=['Malgun Gothic', 'Segoe UI Emoji'])
        init.plt.rcParams['axes.unicode_minus'] = False
        canvas = init.plt.imshow(img)
        init.plt.axis('off')
        init.plt.title("ê·€ì—¬ìš´ ê³ ì–‘ì´ ì‚¬ì§„! ğŸ˜º")
        init.plt.show()
        
    else:
        print(f"ì´ë¯¸ì§€ íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {image_path}")
        
        


def HWPask2(file_path, QA, model = "gpt-5", prompt = None, k = 3):
    loader = HWP.HWP(file_path= file_path)
    context = loader.load()
    idx = 0
    for c in context:
        if c.page_content == None:
            context.pop(idx)
    original_context = ""
    for c in context:
        original_context += str(c)
    
    context = original_context
            

    text_splitter = init.RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 50)
    text_chunks = text_splitter.split_text(context)
    # ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ë¥¼ Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    split_docs = [init.Document(page_content=chunk, metadata={"source": file_path}) for chunk in text_chunks]
    #vectorstore = init.FAISS.from_documents(documents = split_docs, embedding = init.OpenAIEmbeddings(model = "text-embedding-3-large"))
    
    bm25_retriever = init.BM25Retriever.from_documents(split_docs)    
    bm25_retriever.k = k
    faiss_vectorstore = init.FAISS.from_documents(split_docs, init.OpenAIEmbeddings(model="text-embedding-3-large"))
    faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs = {"k" : k})
    #ì•™ìƒë¸” ë¦¬íŠ¸ë¦¬ë²„ë¥¼ ì´ˆê¸°í™”
    ensemble_retiever = init.EnsembleRetriever(
        retrievers=[bm25_retriever, faiss_retriever], weight = [0.5, 0.5]
    )
    
    if prompt == None:
        prompt = init.hub.pull("rlm/rag-prompt") #í”„ë¡¬í”„íŠ¸
    
    prompt += "\n answer in korean"
    
    
    llm = init.ChatOpenAI(model_name = model)
    
    rag_chain = (
        {"context" : ensemble_retiever | format_docs, "question" : init.RunnablePassthrough()}
        #{"context" : init.RunnableLambda(format_docs) | ensemble_retiever, "question" : init.RunnablePassthrough()}
        |prompt
        |llm
        |init.StrOutputParser()
    )
    
    response = rag_chain.invoke(QA)
    response_buff = list()
    response_buff.append(f"HWP Path: {file_path}")
    response_buff.append(f"[HUMAN]\n{QA}\n")
    response_buff.append(f"[AI]\n{response}")
    return response_buff
    
    



        
#set INCLUDE=%INCLUDE%;"A:\vcpkg\installed\x64-windows\include\leptonica"
#set LIB=%LIB%;"A:\vcpkg\installed\x64-windows\lib"  




if __name__ == "__main__":
    # TC.TestClass.test_webBase()
    # TC.TestClass.test_webBase2()
    # TC.TestClass.testJSON()
    # TC.TestClass.testPDF()
    # TC.TestClass.testPPT()
    # loader = CL.csvLoader(file_path = "data/titanic.csv")
    # docs = loader.load()
    # for elem in docs:
    #     print(elem.page_content)
    # QA = "ì‚¼ì„± ê°€ìš°ìŠ¤ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”"
    # file = "data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf"
    # file4 = r"Tensorrt_demos.pdf"
    # file3 = "data/people.json"
    # pdfQuery = PDFask(model = "gpt-5-mini", QA = QA, file_path = file)
    # for elem in pdfQuery:
    #     print(elem)
    # global store
    # store = {}
    # session_id = {'session_id' : 'rag123'}
    # ask = {'system': 'ë‹¹ì‹ ì€ Question-Answering ì±—ë´‡ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.', 'question': 'ì£¼ì–´ì§„ ìë£Œì—ì„œ í•µì‹¬ ì‚¬í•­ì„ ìš”ì•½í•´ì„œ ë…¸ë˜ë¡œ ë§Œë“¤ì–´ ì£¼ì„¸ìš”'}
    # response = simpleChatWithHistory(ask)
    # response = RAG_RunnableWithMessageHistory(file_path = file, ask = ask, session_id= session_id)
    # print(response)
    # import unstructured.partition.pdf
    # print("ì„¤ì¹˜ ì„±ê³µ!")
    
    # docx_loader = WL.docxLoader(file_path="data/sample-word-document.docx")
    # docx_doc = docx_loader.load()
    # print("í™•ì¸")
    # print(docx_doc)
    
    
    # clip = CLIP.CLIP(4000, 0, rf"{file4}", fpath = "data/")
    # chain = clip.load()
    # query = "ë‹¤ìŒ ë¬¸ì„œê°€ ë§í•˜ê³ ìí•˜ëŠ” ë‚´ìš©ì— ëŒ€í•´ì„œ ì‰¬ìš´ ìš©ì–´ë¡œ ì„¤ëª…í•˜ì„¸ìš”.  ê·¸ë¦¬ê³  ìƒì„¸íˆ ë‚´ìš©ì„ ë¶„ì„í•˜ì„¸ìš”. ê·¸ë¦¬ê³  ì£¼ì–´ì§„ ì´ë¯¸ì§€ë“¤ì— ëŒ€í•´ì„œ ì„¤ëª…í•˜ì„¸ìš”. ê·¸ë¦¬ê³  ë‹¹ì‹ ì´ ì •ë¦¬í•œ ëª¨ë“  ë‚´ìš©ì„ ì¼ë³¸ì–´ë¡œ ë²ˆì—­í•˜ì„¸ìš”"
    # response = chain.invoke(query, limit = 6)
    # print(response)
    # test_json = JL.jsonLoader(file_path = file3, jq_schema=".[].phoneNumbers", text_content= False)
    # text_splitter = init.RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 50)
    # print(test_json.load())
    
    # json_response = JSONask(file_path = file3, jq_schema= ".[].phoneNumbers", QA = "ë‹¤ìŒ ë¬¸ì„œë¥¼ ë¶„ì„í•˜ì—¬ ìš”ì•½í•˜ì„¸ìš”")
    # print(json_response)
    custom_prompt = prompt_maker()
    # if init.platform.system() == "Windows": 
    #     hwp = HL.hwpLoader(r"Q:\Coding\PickCareRAG\ë””ì§€í„¸ ì •ë¶€í˜ì‹  ì¶”ì§„ê³„íš.hwp")
    #     docs = hwp.load2()
    #     print(docs)
        
    #     hwp_response = HWPask(file_path= r"Q:\Coding\PickCareRAG\ë””ì§€í„¸ ì •ë¶€í˜ì‹  ì¶”ì§„ê³„íš.hwp",prompt = custom_prompt,  QA = "í•´ë‹¹ ë¬¸ì„œë¥¼ ê³µë¶€í•˜ë¼ê³  ì”ì†Œë¦¬ë§Œ í•´ëŒ€ì„œ ì§‘ì„ ë›°ì³ë‚˜ê°€ì„œ ë°©ë‘í•˜ëŠ” í•œì°¸ ì§ˆí’ë…¸ë„ì˜ ì‹œê¸° ì†ì— ë†“ì—¬ì§„ ë°•í…Œë¦¬ì•„ë„ ì´í•´í•  ìˆ˜ ìˆì„ ë§Œí¼ ì‰½ê²Œ ì •ë¦¬í•˜ì„¸ìš”")
    #     for elem in hwp_response:
    #         print(elem, end = "", flush = True)
    
    # prompt = init.hub.pull("rlm/rag-prompt")
    # print(prompt)
    # if init.platform.system() == "Windows": 
    #     print("ìœˆë„ìš°")
    #     pdf_response = PDFask(file_path=r"Q:\Coding\PickCareRAG\data\Tensorrt_demos.pdf", model = "gpt-5",  QA = "í•´ë‹¹ ë¬¸ì„œë¥¼ ê¸¸ê°€ë˜ ì°¸ìƒˆë„ ì´í•´í•  ìˆ˜ ìˆì„ ë§Œí¼ ì‰½ê²Œ ì •ë¦¬í•˜ì„¸ìš”", prompt=custom_prompt)
    # elif init.platform.system() == "Linux":
    #     print("ë¦¬ëˆ…ìŠ¤")
    #     pdf_response = PDFask(file_path=r"/mnt/q/Coding/PickCareRAG/data/Tensorrt_demos.pdf", model = "gpt-5",  QA = "í•´ë‹¹ ë¬¸ì„œë¥¼ ì¶©ì¹˜ì— ê±¸ë¦° ë‹¤ëŒì¥ë„ ì´í•´í•  ìˆ˜ ìˆì„ ë§Œí¼ ì‰½ê²Œ ì •ë¦¬í•˜ì„¸ìš”", prompt=custom_prompt)
    # for elem in pdf_response:
    #     print(elem, end = "", flush = True)
    
    # create_diffusion_image("cute_cat", "very very cute, lovely, precious kitty staring at me")
        
    
    # create_image(prompt = "A photo of a cute kitten in Kawhi asking for a snack. An animation from the 1980s in Japan", file_name = "output_images/lovely_cat.png")
    
    # create_3d_from_image(file_path= "data/doggum.jpg", output_file = "cutecat.mp4")
    
    hwp_response = HWPask2(file_path= r"Q:\Coding\PickCareRAG\ë””ì§€í„¸ ì •ë¶€í˜ì‹  ì¶”ì§„ê³„íš.hwp",prompt = custom_prompt,  QA = "í•´ë‹¹ ë¬¸ì„œë¥¼ ê³µë¶€í•˜ë¼ê³  ì”ì†Œë¦¬ë§Œ í•´ëŒ€ì„œ ì§‘ì„ ë›°ì³ë‚˜ê°€ì„œ ë°©ë‘í•˜ëŠ” í•œì°¸ ì§ˆí’ë…¸ë„ì˜ ì‹œê¸° ì†ì— ë†“ì—¬ì§„ ë°•í…Œë¦¬ì•„ë„ ì´í•´í•  ìˆ˜ ìˆì„ ë§Œí¼ ì‰½ê²Œ ì •ë¦¬í•˜ì„¸ìš”")
    for elem in hwp_response:
        print(elem)

    

    

            

