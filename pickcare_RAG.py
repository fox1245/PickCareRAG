import asyncio
import logging
from mcp.server.fastmcp import FastMCP
import HWP_async as HWP
import pdfLoader2 as PDF  # ê¸°ì¡´ ëª¨ë“ˆ ìœ ì§€
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.retrievers import BM25Retriever
from langchain_community.vectorstores import FAISS
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.retrievers import EnsembleRetriever
from langchain import hub
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from pathlib import Path
from dotenv import load_dotenv
from langchain_core.documents import Document
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnableLambda
from langchain_core.runnables import RunnableMap
from datetime import datetime
import os
import re
import json
from jsonLoader import jsonLoader
from fastapi.middleware.cors import CORSMiddleware
from fastapi import FastAPI, HTTPException, Body
from pydantic import BaseModel
from typing import Optional, List
from langchain_core.messages import HumanMessage  # langgraphë¥¼ ìœ„í•œ import ì¶”ê°€

# ë¡œê¹… ì„¤ì • (ìƒì„¸ ë¡œê·¸ ê°•í™”)
logging.basicConfig(
    level=logging.DEBUG,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[logging.FileHandler("test_mcp4.log", encoding='utf-8'),  # íŒŒì¼ utf-8
              logging.StreamHandler()],  # ì½˜ì†” ìœ ì§€
)
logger = logging.getLogger(__name__)

SHARED_FOLDER_PATH = os.getenv("SHARED_FOLDER_PATH", r"Q:\Coding\PickCareRAG\shared")  #

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
logger.info("í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ì™„ë£Œ")

mcp = FastMCP("Simple RAG")
retriever_cache = {}  # ê¸°ì¡´ ìºì‹œ ìœ ì§€
vectorstore_cache = {}  # ì‹ ê·œ: vectorstore ìºì‹œ (hash ì—ëŸ¬ í”¼í•¨)

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

# ìˆ˜ì •: ê¸€ë¡œë²Œ RunnableLambdaë¡œ ì •ì˜í•´ì„œ ëª¨ë“  ë„êµ¬ì—ì„œ ê³µìœ 
format_docs_runnable = RunnableLambda(format_docs)

def flatten_json(data, prefix=''):  # ì¶”ê°€: í”Œë˜íŠ¼ í—¬í¼ í•¨ìˆ˜ (ì¬ê·€, ì¸í„°ë„· ì—†ì´ êµ¬í˜„)
    """JSONì„ í‰íƒ„í™”í•˜ëŠ” í—¬í¼. ì¤‘ì²©ì„ 'key.subkey'ë¡œ ë³€í™˜."""
    flat = {}
    if isinstance(data, dict):
        for key, value in data.items():
            new_key = f"{prefix}.{key}" if prefix else key
            if isinstance(value, (dict, list)):
                flat.update(flatten_json(value, new_key))
            else:
                flat[new_key] = value
    elif isinstance(data, list):
        for i, item in enumerate(data):
            new_key = f"{prefix}[{i}]"
            flat.update(flatten_json(item, new_key))
    else:
        flat[prefix] = data
    return flat

@mcp.tool()
async def JSONask(file_path: str, QA: str, jq_schema: str = None, prompt: str = None, k: int = 3, text_content: bool = False, model: str = "gpt-5-mini") -> str:
    """JSON íŒŒì¼ì„ ì²˜ë¦¬í•˜ëŠ” ë„êµ¬. jq_schema ì˜µì…˜(ê¸°ë³¸ None: í”Œë˜íŠ¼ ëª¨ë“œ). RAGë¡œ QA ë‹µë³€."""
    try:
        file_path = Path(file_path).resolve()
        if not file_path.exists() or file_path.suffix.lower() != '.json':
            folder_path = Path(SHARED_FOLDER_PATH).resolve()
            matching_files = [f for f in folder_path.glob('*.json') if 'pet' in f.name.lower() or 'í«' in f.name.lower()]
            if matching_files:
                file_path = matching_files[0]  # ì²« ë²ˆì§¸ ë§¤ì¹­ íŒŒì¼ ì‚¬ìš©
                logger.warning(f"Fallback to shared JSON: {file_path}")
            else:
                raise ValueError(f"Invalid JSON file: {file_path}. ê³µìœ  í´ë”ì—ë„ ì—†ìŒ.")
        logger.info(f"Processing JSON: {file_path}")

        cache_key = str(file_path)
        if cache_key in retriever_cache and retriever_cache[cache_key] is not None:
            ensemble_retriever = retriever_cache[cache_key]
            logger.info("Using cached retriever")
        else:
            # jq_schema ì—†ìœ¼ë©´ í”Œë˜íŠ¼ ëª¨ë“œ
            if jq_schema is None:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                flat_data = flatten_json(data)
                # í”Œë˜íŠ¼ ê²°ê³¼ë¥¼ ë¬¸ìì—´ë¡œ (key: value í˜•ì‹)
                json_text = "\n".join(f"{k}: {v}" for k, v in flat_data.items())
                text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)
                split_docs = text_splitter.split_text(json_text)
                split_docs = [Document(page_content=chunk, metadata={"source": str(file_path)}) for chunk in split_docs]
            else:
                # ê¸°ì¡´ JSONLoader ì‚¬ìš©
                loader = jsonLoader(file_path=str(file_path), jq_schema=jq_schema, text_content=text_content)
                split_docs = await loader.load_and_split(text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50))

            logger.info(f"Split into {len(split_docs)} chunks")

            bm25_retriever = BM25Retriever.from_documents(split_docs)
            bm25_retriever.k = k

            faiss_vectorstore = create_vectorstore(str(file_path), split_docs)
            faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={"k": k})

            ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5])
            retriever_cache[cache_key] = ensemble_retriever
            logger.info("Created and cached new retriever")

        if prompt is None:
            prompt = await asyncio.to_thread(hub.pull, "rlm/rag-prompt")

        llm = ChatOpenAI(model_name=model, timeout=30, max_retries=2)
        rag_chain = (
            {"context": ensemble_retriever | format_docs_runnable, "question": RunnablePassthrough()}
            | prompt
            | llm
            | StrOutputParser()
        )

        async with asyncio.timeout(45):
            response = await rag_chain.ainvoke(QA)

        response_buff = [
            f"JSON Path: {file_path}",
            f"[HUMAN]\n{QA}\n",
            f"[AI]\n{response}"
        ]
        result = "\n".join(response_buff)
        logger.info(f"JSONask ê²°ê³¼: {result}")
        return result

    except Exception as e:
        logger.error(f"Error in JSONask: {e}", exc_info=True)
        raise ValueError(f"JSON ì²˜ë¦¬ ì¤‘ ì—ëŸ¬: {e}")

@mcp.tool()
async def GetCurrentDate() -> str:
    """í˜„ì¬ ì‹œìŠ¤í…œ ë‚ ì§œë¥¼ ë°˜í™˜í•˜ëŠ” ë„êµ¬. ë‚ ì§œ ê´€ë ¨ ì§ˆë¬¸ ì‹œ ì‚¬ìš©. ë°˜í™˜ í˜•ì‹: YYYY-MM-DD (ì˜ˆ: 2025-08-15)."""
    try:
        current_date = datetime.now().strftime("%Y-%m-%d")
        logger.info(f"GetCurrentDate í˜¸ì¶œ: {current_date}")
        return current_date
    except Exception as e:
        logger.error(f"GetCurrentDate ì—ëŸ¬: {e}", exc_info=True)
        raise ValueError(f"ë‚ ì§œ ê°€ì ¸ì˜¤ê¸° ì¤‘ ì—ëŸ¬: {e}")

@mcp.tool()
async def prompt_maker():  # async ìœ ì§€ (MCP ë„êµ¬ ê·œì¹™)
    """ê³ ì–‘ì´ ë§íˆ¬ë¡œ ëŒ€í™”í•  ìˆë„ë¡ í•˜ëŠ” í”„ë¡¬í”„íŠ¸ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ì´ë‹¤. PDFaskì˜ prompt ì¸ìë‚˜ HWPaskì˜ promptì¸ìì— í• ë‹¹í•˜ê²Œ ë˜ë©´ ê³ ì–‘ì´ ë§íˆ¬ë¥¼ ì‚¬ìš©í•œë‹¤."""
    custom_prompt = PromptTemplate(
        input_variables=["context", "question"],
        template="ëƒ¥! ì €ëŠ” ë¬¸ì„œë¥¼ ì½ê³  ë§í•  ì¤„ ì•„ëŠ” ë˜‘ë˜‘í•œ ê³ ì–‘ì´ì˜ˆìš”~ ğŸ˜º\n{context}ë¥¼ ë³´ê³ , {question}ì— ëŒ€í•´ ìµœëŒ€í•œ ê·€ì—½ê³  ì‚¬ë‘ìŠ¤ëŸ½ê³  ì¹´ì™€ì´í•œ ê³ ì–‘ì´ ë§íˆ¬ë¡œ ì •ë¦¬í•´ì¤„ê²Œìš”! ì•¼ì˜¹~ ë‹µë³€ì€ ì•„ì£¼ ë””í…Œì¼í•˜ê³  ë‚´ ì„¬ì„¸í•œ ìˆ˜ì—¼ì²˜ëŸ¼ ì´ˆ~ ì„¼ì„œí‹°ë¸Œí•˜ê²Œ ë‹µë³€í•´ì¤„ê²Œ ëƒ¥ëƒ¥. ë‹µë³€ì´ ë§Œì¡±ìŠ¤ëŸ¬ìš°ë©´ ê³ ê¸‰ ì¸„ë¥´ í•œ ê°œ ì¤„ë˜ëƒ¥?. \në‹µë³€: ",
    )
    return custom_prompt  # ìˆ˜ì •: await ì œê±°, returnìœ¼ë¡œ ì§ì ‘ ë°˜í™˜

def create_vectorstore(file_path: str, split_docs: list[Document]):  # ìˆ˜ì •: lru_cache ì œê±°, listë¡œ ë³€ê²½, ìºì‹œ dict ì‚¬ìš©
    cache_key = file_path  # file_pathë§Œìœ¼ë¡œ ìºì‹± (split_docsëŠ” deterministic ê°€ì •)
    if cache_key in vectorstore_cache:
        logger.debug(f"Vectorstore cache hit for {file_path}")
        return vectorstore_cache[cache_key]
    else:
        embeddings = OpenAIEmbeddings(model="text-embedding-3-large")
        logger.debug(f"Creating vectorstore for {file_path}")
        vs = FAISS.from_documents(split_docs, embeddings)
        vectorstore_cache[cache_key] = vs
        return vs

@mcp.tool()
async def SharedFolderSearchAndRAG(QA: str, prompt: str = None, file_types: str = ".pdf,.hwp,.hwpx,.json", history: str = None) -> str:
    """ê³µìœ  í´ë”ë¥¼ ììœ¨ ê²€ìƒ‰í•´ ì í•© íŒŒì¼ ì„ íƒ í›„ RAG ì§„í–‰. file_typesëŠ” comma-separated (e.g., .pdf,.hwp, .json)."""
    try:
        folder_path = Path(SHARED_FOLDER_PATH).resolve()
        if not folder_path.exists() or not folder_path.is_dir():
            raise ValueError(f"ê³µìœ  í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•Šê±°ë‚˜ ë””ë ‰í† ë¦¬ê°€ ì•„ë‹™ë‹ˆë‹¤: {folder_path}")
        
        # ë¹„ë™ê¸° íŒŒì¼ ëª©ë¡ ìˆ˜ì§‘
        files = []
        async def scan_files():
            for entry in await asyncio.to_thread(os.scandir, folder_path):
                if entry.is_file() and any(entry.name.lower().endswith(ext) for ext in file_types.split(",")):
                    files.append({"path": entry.path, "name": entry.name, "mtime": entry.stat().st_mtime})
        await scan_files()
        if not files:
            raise ValueError("ì í•©í•œ íŒŒì¼(.pdf, .hwp)ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        logger.info(f"ê³µìœ  í´ë”ì—ì„œ {len(files)}ê°œ íŒŒì¼ ë°œê²¬: {[f['name'] for f in files]}")
        
        # ìˆ˜ì •: historyê°€ ìˆìœ¼ë©´ QAì— ë¶™ì„ (ëŒ€í™” ë§¥ë½ ë°˜ì˜ìœ¼ë¡œ íŒŒì¼ ì„ íƒ ì •í™•ë„ â†‘)
        if history:
            QA = f"{history}\n{QA}"
        
        # ì—ì´ì „íŠ¸ê°€ ì í•© íŒŒì¼ ì„ íƒ (LLM íŒë‹¨)
        select_llm = ChatOpenAI(model_name=os.getenv("OPENAI_MODEL_NAME", "gpt-5-mini"))
        # íŒŒì¼ ì„ íƒ í”„ë¡¬í”„íŠ¸ ìˆ˜ì •: history ë°˜ì˜ (ì´ì „ íˆìŠ¤í† ë¦¬ë¥¼ ëª…ì‹œì ìœ¼ë¡œ í¬í•¨í•´ LLMì´ ë§¥ë½ ì´í•´)
        select_prompt = f"ì´ì „ íˆìŠ¤í† ë¦¬: {history or 'ì—†ìŒ'}\nQA í‚¤ì›Œë“œ: {QA}\níŒŒì¼ ëª©ë¡: {[f['name'] + ' (full path: ' + str(Path(folder_path) / f['name']) + ', modified: ' + str(f['mtime']) + ')' for f in files]}\nQA í‚¤ì›Œë“œì™€ ê°€ì¥ ë§¤ì¹­ë˜ëŠ” íŒŒì¼ì˜ FULL ABSOLUTE PATH ì„ íƒ (e.g., 'ê³ ì–‘ì´' in QA â†’ 'Q:\\full\\path\\to\\ê³ ì–‘ì´ì— ëŒ€í•œ 5ê°€ì§€ ë†€ë¼ìš´ ì‚¬ì‹¤.hwp'). ì´ìœ  ì„¤ëª…í•˜ê³ , í˜•ì‹: Selected Full Path: Q:\\full\\path\\to\\file.ext"
        select_response = await select_llm.ainvoke(select_prompt)
        match = re.search(r"Selected Full Path: (.*)", select_response.content, re.IGNORECASE)
        selected_file_path = match.group(1).strip() if match else str(Path(folder_path) / files[0]['name'])
        if not Path(selected_file_path).exists():
            logger.warning(f"Selected path not exists: {selected_file_path}. Falling back.")
            selected_file_name = Path(selected_file_path).name if Path(selected_file_path).name else files[0]['name']
            selected_file_path = str(Path(folder_path) / selected_file_name)
        logger.info(f"ì„ íƒëœ íŒŒì¼: {selected_file_path}")
        
        # íŒŒì¼ í™•ì¥ìì— ë”°ë¼ ë¡œë” ì„ íƒ
        selected_path = Path(selected_file_path)
        if selected_path.suffix.lower() == '.pdf':
            loader = PDF.pdfLoader(file_path=selected_file_path, extract_bool=True)
            split_docs = await loader.load_and_split(text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50))
        elif selected_path.suffix.lower() in ('.hwp'):
            loader = HWP.HWP(file_path=selected_file_path)
            context = await loader.load()
            idx = 0
            while idx < len(context):
                if context[idx].page_content is None:
                    context.pop(idx)
                else:
                    idx += 1
            original_context = "".join(str(c) for c in context)
            text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)
            text_chunks = text_splitter.split_text(original_context)
            split_docs = [Document(page_content=chunk, metadata={"source": str(selected_file_path)}) for chunk in text_chunks]
            
        elif selected_path.suffix.lower() == '.json':
            # JSON ì²˜ë¦¬: jq_schema Noneìœ¼ë¡œ í”Œë˜íŠ¼ ëª¨ë“œ ì‚¬ìš© (JSONask ë¡œì§ ì¬í™œìš©)
            with open(selected_file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            flat_data = flatten_json(data)
            json_text = "\n".join(f"{k}: {v}" for k, v in flat_data.items())
            text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)
            text_chunks = text_splitter.split_text(json_text)
            split_docs = [Document(page_content=chunk, metadata={"source": str(selected_file_path)}) for chunk in text_chunks]
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {selected_path.suffix}")
        
        logger.info(f"Split into {len(split_docs)} chunks")
        
        # ê¸°ì¡´ RAG í”„ë¡œì„¸ìŠ¤
        cache_key = str(selected_file_path)
        if cache_key in retriever_cache and retriever_cache[cache_key] is not None:
            ensemble_retriever = retriever_cache[cache_key]
            logger.info("Using cached retriever")
        else:
            bm25_retriever = BM25Retriever.from_documents(split_docs)
            bm25_retriever.k = 3
            faiss_vectorstore = create_vectorstore(selected_file_path, split_docs)  # ìˆ˜ì •: list ì „ë‹¬, ìºì‹œ dict ì‚¬ìš©
            faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={"k": 3})
            ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5])
            retriever_cache[cache_key] = ensemble_retriever
            logger.info("Created and cached new retriever")
        
        if prompt is None:
            prompt = await asyncio.to_thread(hub.pull, "rlm/rag-prompt")
        elif isinstance(prompt, str):
            prompt = PromptTemplate.from_template(prompt)
        elif not isinstance(prompt, PromptTemplate):
            logger.error(f"Invalid prompt type: {type(prompt)}. Falling back to default.")
            prompt = await asyncio.to_thread(hub.pull, "rlm/rag-prompt")
        llm = ChatOpenAI(model_name=os.getenv("OPENAI_MODEL_NAME", "gpt-5-mini"), timeout=60, max_retries=2)
        logger.debug(f"Prompt type after processing: {type(prompt)}")
        rag_chain = RunnableMap(
            {"context": ensemble_retriever | format_docs_runnable, "question": RunnablePassthrough()}
        ) | prompt | llm | StrOutputParser()
        
        async with asyncio.timeout(60):
            response = await rag_chain.ainvoke(QA)
        
        response_buff = [
            f"Selected File: {selected_file_path}",
            f"[HUMAN]\n{QA}\n",
            f"[AI]\n{response}"
        ]
        result = "\n".join(response_buff)
        logger.info(f"SharedFolderSearchAndRAG ê²°ê³¼: {result}")
        return result
    
    except Exception as e:
        logger.error(f"SharedFolderSearchAndRAG ì—ëŸ¬: {e}", exc_info=True)
        if "unhashable" in str(e).lower():
            logger.error("Unhashable Document detected - Consider serializing split_docs for caching.")
        raise ValueError(f"ê³µìœ  í´ë” ì²˜ë¦¬ ì¤‘ ì—ëŸ¬: {e}. ê²½ë¡œ/ê¶Œí•œ í™•ì¸í•˜ì„¸ìš”.")

@mcp.tool()
async def PDFask(file_path: str, QA: str, prompt: str = None) -> str:
    try:
        file_path = Path(file_path).resolve()
        if not file_path.exists() or file_path.suffix.lower() != '.pdf':
            raise ValueError(f"Invalid PDF file: {file_path}")
        logger.info(f"Processing PDF: {file_path}")

        cache_key = str(file_path)
        if cache_key in retriever_cache and retriever_cache[cache_key] is not None:
            ensemble_retriever = retriever_cache[cache_key]
            logger.info("Using cached retriever")
        else:
            loader = PDF.pdfLoader(file_path=file_path, extract_bool=False)
            split_docs = await loader.load_and_split(
                text_splitter=RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
            )
            logger.info(f"Split into {len(split_docs)} chunks")

            bm25_retriever = BM25Retriever.from_documents(split_docs)
            bm25_retriever.k = 3

            faiss_vectorstore = create_vectorstore(file_path, split_docs)  # ìˆ˜ì •: list ì „ë‹¬
            logger.debug("FAISS vectorstore created successfully")
            faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={"k": 3})

            ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5])
            retriever_cache[cache_key] = ensemble_retriever
            logger.info("Created and cached new retriever")

        if prompt is None:
            prompt = await asyncio.to_thread(hub.pull, "rlm/rag-prompt")

        llm = ChatOpenAI(
            model_name="gpt-5-mini",
            timeout=30,
            max_retries=2
        )
        rag_chain = (
            {"context": ensemble_retriever | format_docs_runnable, "question": RunnablePassthrough()}
            | prompt
            | llm
            | StrOutputParser()
        )

        async with asyncio.timeout(45):
            logger.debug(f"Invoking rag_chain with QA: {QA}")
            response = await rag_chain.ainvoke(QA)
            logger.debug(f"rag_chain response: {response}")

        response_buff = [
            f"PDF Path: {file_path}",
            f"[HUMAN]\n{QA}\n",
            f"[AI]\n{response}"
        ]
        result = "\n".join(response_buff)
        logger.info(f"PDFask ê²°ê³¼: {result}")
        return result

    except asyncio.TimeoutError:
        logger.error(f"Timeout in PDFask for {file_path}")
        raise ValueError("PDF processing or LLM call timed out")
    except TypeError as te:
        logger.error(f"Chain operator error in vectorstore: {te} â€“ Falling back to non-cached creation")
        # fallback: ì§ì ‘ ìƒì„±
        faiss_vectorstore = FAISS.from_documents(split_docs, OpenAIEmbeddings(model="text-embedding-3-large"))
    except Exception as e:
        logger.error(f"Error in PDFask: {e}", exc_info=True)
        raise

@mcp.tool()
async def HWPask(file_path: str, QA: str, prompt: str = None) -> str:
    try:
        file_path = Path(file_path).resolve()
        if not file_path.exists():
            logger.error(f"File does not exist: {file_path} - Check path and rename if spaces present.")
            raise ValueError(f"File does not exist: {file_path}. Please verify the path and remove spaces if any.")
        if file_path.suffix.lower() not in ('.hwp', '.hwpx'):
            raise ValueError(f"Invalid HWP or HWPX file: {file_path}")
        logger.info(f"Processing HWP: {file_path}")
        logger.debug(f"File confirmed exists: {file_path.exists()}, Suffix: {file_path.suffix.lower()}")

        cache_key = str(file_path)
        if cache_key in retriever_cache and retriever_cache[cache_key] is not None:
            ensemble_retriever = retriever_cache[cache_key]
            logger.info("Using valid cached retriever")
        else:
            loader = HWP.HWP(file_path=file_path)
            context = await loader.load()
            if not context:
                raise ValueError("Loaded context is empty - Check HWPLoader or file integrity.")
            idx = 0
            while idx < len(context):
                if context[idx].page_content is None:
                    context.pop(idx)
                else:
                    idx += 1
            original_context = "".join(str(c) for c in context)
            text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)
            text_chunks = text_splitter.split_text(original_context)
            split_docs = [Document(page_content=chunk, metadata={"source": str(file_path)}) for chunk in text_chunks]
            logger.info(f"Split into {len(split_docs)} chunks")
            
            bm25_retriever = BM25Retriever.from_documents(split_docs)
            bm25_retriever.k = 3
            
            faiss_vectorstore = create_vectorstore(file_path, split_docs)  # ìˆ˜ì •: list ì „ë‹¬
            logger.debug("FAISS vectorstore created successfully")
            faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={"k": 3})
            ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5])
            retriever_cache[cache_key] = ensemble_retriever
            logger.info("Created and cached new retriever")

        if prompt is None:
            prompt = await asyncio.to_thread(hub.pull, "rlm/rag-prompt")
            
        llm = ChatOpenAI(
            model_name="gpt-5-mini",
            timeout=30,
            max_retries=2
        )
        
        rag_chain = (
            {"context": ensemble_retriever | format_docs_runnable, "question": RunnablePassthrough()}
            | prompt
            | llm
            | StrOutputParser()
        )
        
        async with asyncio.timeout(45):
            logger.debug(f"Invoking rag_chain with QA: {QA}")
            response = await rag_chain.ainvoke(QA)
            logger.debug(f"rag_chain response: {response}")

        response_buff = [
            f"HWP Path: {file_path}",
            f"[HUMAN]\n{QA}\n",
            f"[AI]\n{response}"
        ]
        result = "\n".join(response_buff)
        logger.info(f"HWPask ê²°ê³¼: {result}")
        return result

    except ValueError as ve:
        logger.error(f"ValueError in HWPask: {ve}", exc_info=True)
        return f"Error: {ve} - Please check file path and try again."
    except Exception as e:
        logger.error(f"Unexpected error in HWPask: {e}", exc_info=True)
        raise  

app = FastAPI(title="RAG MCP API", description="RESTful API for RAG with MCP tools")
# app = FastAPI(...) ë°”ë¡œ ì•„ë˜ì— ì¶”ê°€
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ê°œë°œìš© ëª¨ë“  ì˜¤ë¦¬ì§„ í—ˆìš©. í”„ë¡œë•ì…˜ ì‹œ ["http://your-frontend-domain.com"]ë¡œ ì œí•œ
    allow_credentials=True,
    allow_methods=["*"],  # OPTIONS, POST ë“± ëª¨ë“  ë©”ì„œë“œ í—ˆìš©
    allow_headers=["*"],  # Content-Type ë“± ëª¨ë“  í—¤ë” í—ˆìš©
)


# Pydantic ëª¨ë¸: JSON í˜ì´ë¡œë“œ ì •ì˜ (ì¿¼ë¦¬ ì…ë ¥ ìœ íš¨ì„± ê²€ì‚¬)
class QueryRequest(BaseModel):
    query: str  # ì‚¬ìš©ì ì¿¼ë¦¬ (í•„ìˆ˜)
    file_path: Optional[str] = None  # ì˜µì…˜: íŒŒì¼ ê²½ë¡œ (PDFask ë“±ì— ì‚¬ìš©)
    tool: Optional[str] = None  # ì˜µì…˜: í˜¸ì¶œí•  MCP ë„êµ¬ ì´ë¦„ (e.g., "PDFask")
    history: Optional[List[dict]] = None  # ì˜µì…˜: ëŒ€í™” íˆìŠ¤í† ë¦¬ (clientì²˜ëŸ¼ ìš”ì•½ ì „ë‹¬)

class QueryResponse(BaseModel):
    result: str  # RAG ê²°ê³¼
    source: Optional[str] = None  # ì‚¬ìš©ëœ íŒŒì¼/ë„êµ¬ ì†ŒìŠ¤
    timestamp: str  # ì²˜ë¦¬ ì‹œê°„

# RESTful ì—”ë“œí¬ì¸íŠ¸: POST /api/rag/query (ì£¼ ì—”ë“œí¬ì¸íŠ¸)
@app.post("/api/rag/query", response_model=QueryResponse)
async def rag_query(request: QueryRequest = Body(...)):
    try:
        logger.info(f"Received query: {request.query}")
        # MCP í˜¸ì¶œ ë¡œì§: ì¿¼ë¦¬ì— ë”°ë¼ ë„êµ¬ ì„ íƒ (ê¸°ì¡´ agentì²˜ëŸ¼)
        # ì˜ˆ: SharedFolderSearchAndRAG ë„êµ¬ í˜¸ì¶œ (history í¬í•¨)
        if request.tool == "SharedFolderSearchAndRAG":
            result = await SharedFolderSearchAndRAG(
                QA=request.query,
                history="\n".join([f"{msg['role']}: {msg['content']}" for msg in request.history or []]) if request.history else None
            )
        elif request.tool == "PDFask":
            if not request.file_path:
                raise HTTPException(status_code=400, detail="file_path required for PDFask")
            result = await PDFask(file_path=request.file_path, QA=request.query)
        elif request.tool == "HWPask":
            if not request.file_path:
                raise HTTPException(status_code=400, detail="file_path required for HWPask")
            result = await HWPask(file_path=request.file_path, QA=request.query)
        elif request.tool == "JSONask":
            if not request.file_path:
                raise HTTPException(status_code=400, detail="file_path required for JSONask")
            result = await JSONask(file_path=request.file_path, QA=request.query)
        # ... (ë‹¤ë¥¸ ë„êµ¬ë“¤ ì¶”ê°€: GetCurrentDate, prompt_maker ë“± í•„ìš” ì‹œ if-elif í™•ì¥)
        else:
            # ê¸°ë³¸: ì—ì´ì „íŠ¸ì²˜ëŸ¼ MCP ì „ì²´ í˜¸ì¶œ (PleaseBE5_client.py ìŠ¤íƒ€ì¼)
            from langgraph.prebuilt import create_react_agent  # import ì¶”ê°€ (ëŸ°íƒ€ì„ì— ë¶ˆëŸ¬ì„œ ë©”ëª¨ë¦¬ ì ˆì•½)
            tools = await mcp.get_tools()  # MCP ë„êµ¬ë“¤
            agent = create_react_agent(model="openai:gpt-5-mini", tools=tools)
            input_message = {"messages": [HumanMessage(content=request.query)]}
            agent_response = await agent.ainvoke(input_message)
            result = agent_response["messages"][-1].content

        return QueryResponse(
            result=result,
            source=request.file_path or "Shared Folder",
            timestamp=datetime.now().isoformat()
        )
    except Exception as e:
        logger.error(f"Error in rag_query: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ì¶”ê°€ ì—”ë“œí¬ì¸íŠ¸: GET /api/health (í—¬ìŠ¤ ì²´í¬)
@app.get("/api/health")
async def health_check():
    return {"status": "healthy", "version": "1.0"}

# MCP ë„êµ¬ ëª©ë¡ ë…¸ì¶œ: GET /api/tools
@app.get("/api/tools")
async def get_tools():
    tools = await mcp.get_tools()
    return {"tools": [tool.name for tool in tools]}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)  # ë¡œì»¬ í…ŒìŠ¤íŠ¸