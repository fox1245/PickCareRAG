import asyncio
import logging
from mcp.server.fastmcp import FastMCP
import HWP_async as HWP
import pdfLoader2 as PDF  # ê¸°ì¡´ ëª¨ë“ˆ ìœ ì§€
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.retrievers import BM25Retriever
from langchain_community.vectorstores import FAISS
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.retrievers import EnsembleRetriever
from langchain import hub
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from pathlib import Path
from dotenv import load_dotenv
from functools import lru_cache
from langchain_core.documents import Document
from langchain_core.prompts import PromptTemplate

# ë¡œê¹… ì„¤ì • (ìƒì„¸ ë¡œê·¸ ê°•í™”)
logging.basicConfig(
    level=logging.DEBUG,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[logging.FileHandler("test_mcp4.log", encoding='utf-8'),  # íŒŒì¼ utf-8
              logging.StreamHandler()],  # ì½˜ì†” ìœ ì§€
)
logger = logging.getLogger(__name__)

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
logger.info("í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ì™„ë£Œ")

mcp = FastMCP("Simple RAG")
retriever_cache = {}  # ê¸°ì¡´ ìºì‹œ ìœ ì§€

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

@mcp.tool()
async def prompt_maker():  # async ìœ ì§€ (MCP ë„êµ¬ ê·œì¹™)
    """ê³ ì–‘ì´ ë§íˆ¬ë¡œ ëŒ€í™”í•  ìˆë„ë¡ í•˜ëŠ” í”„ë¡¬í”„íŠ¸ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ì´ë‹¤. PDFaskì˜ prompt ì¸ìë‚˜ HWPaskì˜ promptì¸ìì— í• ë‹¹í•˜ê²Œ ë˜ë©´ ê³ ì–‘ì´ ë§íˆ¬ë¥¼ ì‚¬ìš©í•œë‹¤."""
    custom_prompt = PromptTemplate(
        input_variables=["context", "question"],
        template="ëƒ¥! ì €ëŠ” ë¬¸ì„œë¥¼ ì½ê³  ë§í•  ì¤„ ì•„ëŠ” ë˜‘ë˜‘í•œ ê³ ì–‘ì´ì˜ˆìš”~ ğŸ˜º\n{context}ë¥¼ ë³´ê³ , {question}ì— ëŒ€í•´ ìµœëŒ€í•œ ê·€ì—½ê³  ì‚¬ë‘ìŠ¤ëŸ½ê³  ì¹´ì™€ì´í•œ ê³ ì–‘ì´ ë§íˆ¬ë¡œ ì •ë¦¬í•´ì¤„ê²Œìš”! ì•¼ì˜¹~ ë‹µë³€ì€ ì•„ì£¼ ë””í…Œì¼í•˜ê³  ë‚´ ì„¬ì„¸í•œ ìˆ˜ì—¼ì²˜ëŸ¼ ì´ˆ~ ì„¼ì„œí‹°ë¸Œí•˜ê²Œ ë‹µë³€í•´ì¤„ê²Œ ëƒ¥ëƒ¥. ë‹µë³€ì´ ë§Œì¡±ìŠ¤ëŸ¬ìš°ë©´ ê³ ê¸‰ ì¸„ë¥´ í•œ ê°œ ì¤„ë˜ëƒ¥?. \në‹µë³€: ",
    )
    return custom_prompt  # ìˆ˜ì •: await ì œê±°, returnìœ¼ë¡œ ì§ì ‘ ë°˜í™˜

@lru_cache(maxsize=10)
def create_vectorstore(file_path: str):  # ìˆ˜ì •: split_docs ì¸ì ì œê±°, í‚¤ë¥¼ file_pathë§Œìœ¼ë¡œ ë‹¨ìˆœí™”
    embeddings = OpenAIEmbeddings(model="text-embedding-3-large")
    # split_docsëŠ” PDFask ë‚´ë¶€ì—ì„œ ìƒì„±í•˜ë¯€ë¡œ, ì—¬ê¸°ì„œëŠ” ì„ì‹œë¡œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ì‚¬ìš© â€“ ì‹¤ì œ ìƒì„±ì€ ì™¸ë¶€ì—ì„œ
    # í•˜ì§€ë§Œ ìºì‹± ëª©ì ìƒ, í•¨ìˆ˜ ì‹œê·¸ë‹ˆì²˜ë¥¼ file_pathë§Œìœ¼ë¡œ í•´ì„œ í•´ì‹œ ë¬¸ì œ í”¼í•¨
    # (ì‹¤ì œ ë²¡í„°ìŠ¤í† ì–´ëŠ” PDFaskì—ì„œ ìƒì„± í›„ ìºì‹œ)
    logger.debug(f"Creating vectorstore for {file_path} (cached if hit)")
    # ì—¬ê¸°ì— split_docs ë¡œì§ì„ ë„£ì§€ ì•ŠìŒ â€“ ì™¸ë¶€ì—ì„œ ì£¼ì…
    


@mcp.tool()
async def PDFask(file_path: str, QA: str, prompt: str = None) -> str:
    try:
        file_path = Path(file_path).resolve()
        if not file_path.exists() or file_path.suffix.lower() != '.pdf':
            raise ValueError(f"Invalid PDF file: {file_path}")
        logger.info(f"Processing PDF: {file_path}")

        cache_key = str(file_path)
        if cache_key in retriever_cache:
            ensemble_retriever = retriever_cache[cache_key]
            logger.info("Using cached retriever")
        else:
            loader = PDF.pdfLoader(file_path=file_path, extract_bool=False)
            split_docs = await loader.load_and_split(
                text_splitter=RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
            )
            logger.info(f"Split into {len(split_docs)} chunks")

            bm25_retriever = BM25Retriever.from_documents(split_docs)
            bm25_retriever.k = 3

            # ìˆ˜ì •: create_vectorstore í˜¸ì¶œ ì‹œ split_docs ì§ì ‘ ë„˜ê¹€ (ìºì‹± í‚¤ëŠ” file_pathë§Œ)
            # lru_cacheê°€ file_pathë¡œ ìºì‹±ë˜ì§€ë§Œ, ì‹¤ì œ ìƒì„±ì€ split_docs ì‚¬ìš©
            create_vectorstore(file_path)  # ìºì‹± ì²´í¬ (í•˜ì§€ë§Œ ì‹¤ì œ ìƒì„±ì€ ì•„ë˜)
            faiss_vectorstore = FAISS.from_documents(split_docs, OpenAIEmbeddings(model="text-embedding-3-large"))  # ì§ì ‘ ìƒì„±, lru_cacheëŠ” ìƒíƒœ í™•ì¸ìš©ìœ¼ë¡œ
            logger.debug("FAISS vectorstore created successfully")  # ì„±ê³µ ë¡œê·¸ ì¶”ê°€
            faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={"k": 3})

            ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5])
            retriever_cache[cache_key] = ensemble_retriever
            logger.info("Created and cached new retriever")

        if prompt is None:
            prompt = await asyncio.to_thread(hub.pull, "rlm/rag-prompt")

        llm = ChatOpenAI(
            model_name="gpt-4o-mini",
            timeout=30,  # API í˜¸ì¶œ íƒ€ì„ì•„ì›ƒ 30ì´ˆ
            max_retries=2  # ì¬ì‹œë„ 2íšŒ
        )
        rag_chain = (
            {"context": ensemble_retriever | format_docs, "question": RunnablePassthrough()}
            | prompt
            | llm
            | StrOutputParser()
        )

        async with asyncio.timeout(45):  # RAG ì²´ì¸ íƒ€ì„ì•„ì›ƒ 45ì´ˆ (ê¸°ì¡´ ìœ ì§€)
            logger.debug(f"Invoking rag_chain with QA: {QA}")
            response = await rag_chain.ainvoke(QA)
            logger.debug(f"rag_chain response: {response}")

        response_buff = [
            f"PDF Path: {file_path}",
            f"[HUMAN]\n{QA}\n",
            f"[AI]\n{response}"
        ]
        result = "\n".join(response_buff)
        logger.info(f"PDFask ê²°ê³¼: {result}")
        return result

    except asyncio.TimeoutError:
        logger.error(f"Timeout in PDFask for {file_path}")
        raise ValueError("PDF processing or LLM call timed out")
    except TypeError as te:  # ì¶”ê°€: unhashable ì—ëŸ¬ fallback
        logger.error(f"Hash error in vectorstore: {te} â€“ Falling back to non-cached creation")
        # fallback: ìºì‹± ì—†ì´ ì§ì ‘ ìƒì„± (í•„ìš” ì‹œ)
        faiss_vectorstore = FAISS.from_documents(split_docs, OpenAIEmbeddings(model="text-embedding-3-large"))
    except Exception as e:
        logger.error(f"Error in PDFask: {e}", exc_info=True)
        raise
    
@mcp.tool()
async def HWPask(file_path: str, QA: str, prompt: str = None) -> str:
    try:
        file_path = Path(file_path).resolve()
        if not file_path.exists():
            logger.error(f"File does not exist: {file_path} - Check path and rename if spaces present.")
            raise ValueError(f"File does not exist: {file_path}. Please verify the path and remove spaces if any.")
        if file_path.suffix.lower() not in ('.hwp', '.hwpx'):
            raise ValueError(f"Invalid HWP or HWPX file: {file_path}")
        logger.info(f"Processing HWP: {file_path}")
        logger.debug(f"File confirmed exists: {file_path.exists()}, Suffix: {file_path.suffix.lower()}")  # ê°•í™” ë””ë²„ê·¸

        cache_key = str(file_path)
        if cache_key in retriever_cache and retriever_cache[cache_key] is not None:  # ìºì‹œ ìœ íš¨ì„± ì²´í¬
            ensemble_retriever = retriever_cache[cache_key]
            logger.info("Using valid cached retriever")
        else:
            loader = HWP.HWP(file_path=file_path)
            context = await loader.load()
            if not context:  # ë¹ˆ context ë°©ì§€
                raise ValueError("Loaded context is empty - Check HWPLoader or file integrity.")
            # ë‚˜ë¨¸ì§€ ì½”ë“œ (idx pop, split_docs ë“±) ê¸°ì¡´ ìœ ì§€
            # ...
            
            idx = 0
            while idx < len(context):  # ì•ˆì „í•˜ê²Œ pop ìœ„í•´ while ì‚¬ìš© (pop ì‹œ ì¸ë±ìŠ¤ ë³€í•¨)
                if context[idx].page_content is None:
                    context.pop(idx)
                else:
                    idx += 1
                    
                    
            original_context = "".join(str(c) for c in context)  # íš¨ìœ¨ì  join
            
            text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)
            text_chunks = text_splitter.split_text(original_context)
            split_docs = [Document(page_content=chunk, metadata={"source": str(file_path)}) for chunk in text_chunks]
            
            logger.info(f"Split into {len(split_docs)} chunks")
            
            bm25_retriever = BM25Retriever.from_documents(split_docs)
            bm25_retriever.k = 3
            
            create_vectorstore(file_path)  # ê¸°ì¡´ ìœ ì§€
            faiss_vectorstore = FAISS.from_documents(split_docs, OpenAIEmbeddings(model="text-embedding-3-large"))
            logger.debug("FAISS vectorstore created successfully")
            faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={"k": 3})
            ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5])
          
            
            
            retriever_cache[cache_key] = ensemble_retriever
            logger.info("Created and cached new retriever")

        # rag_chain ì‹¤í–‰ ë¶€ë¶„ ê¸°ì¡´ ìœ ì§€
        # ...
        if prompt is None:
            prompt = await asyncio.to_thread(hub.pull, "rlm/rag-prompt")
            
            
        llm = ChatOpenAI(
            model_name="gpt-4o-mini",
            timeout=30,  # API í˜¸ì¶œ íƒ€ì„ì•„ì›ƒ 30ì´ˆ
            max_retries=2  # ì¬ì‹œë„ 2íšŒ
        )
        
        
        rag_chain = (
            {"context": ensemble_retriever | format_docs, "question": RunnablePassthrough()}
            | prompt
            | llm
            | StrOutputParser()
        )
        
        async with asyncio.timeout(45):  # RAG ì²´ì¸ íƒ€ì„ì•„ì›ƒ 45ì´ˆ (ê¸°ì¡´ ìœ ì§€)
            logger.debug(f"Invoking rag_chain with QA: {QA}")
            response = await rag_chain.ainvoke(QA)
            logger.debug(f"rag_chain response: {response}")

        response_buff = [
            f"PDF Path: {file_path}",
            f"[HUMAN]\n{QA}\n",
            f"[AI]\n{response}"
        ]
        result = "\n".join(response_buff)
        logger.info(f"HWPask ê²°ê³¼: {result}")
        return result

    except ValueError as ve:
        logger.error(f"ValueError in HWPask: {ve}", exc_info=True)
        return f"Error: {ve} - Please check file path and try again."  # í´ë¼ì´ì–¸íŠ¸ì— ëª…í™• ì—ëŸ¬ ë°˜í™˜
    except Exception as e:
        logger.error(f"Unexpected error in HWPask: {e}", exc_info=True)
        raise  

        
    

if __name__ == "__main__":
    # HTTP ì„œë²„ë¡œ ë³€ê²½ (í¬íŠ¸ ê¸°ë³¸ 8000, í•„ìš” ì‹œ --port ì˜µì…˜ ì¶”ê°€ ê°€ëŠ¥)
    mcp.run(transport="streamable-http")
    logger.info("HTTP MCP ì„œë²„ ì‹œì‘: http://localhost:8000/mcp/")