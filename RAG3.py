import init
import WebBaseLoader as WB
import pdfLoader as PDF
import jsonLoader as JL
import pptLoader as PL
import csvLoader as CL
import CLIP_RAG as CLIP
import docxLoader_old as WL
from test_grok import create_image
import HWP
from langgraph.prebuilt import create_react_agent
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI
from langchain_community.cache import SQLiteCache
from langchain.globals import set_llm_cache
import glob
import os
from pathlib import Path
import logging
from retry import retry
import fitz
import unicodedata
import hashlib


# ìºì‹± ì´ˆê¸°í™”
set_llm_cache(SQLiteCache(database_path=".langchain.db"))

global store
store = {}

# API í‚¤ ì •ë³´ ë¡œë“œ
init.load_dotenv()

# tqdm ê°ì²´ë¥¼ ì „ì—­ ë³€ìˆ˜ë¡œ ì„ ì–¸
progress_bar = None

# ì„¸ì…˜ IDë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì„¸ì…˜ ê¸°ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
def get_session_history(session_ids):
    print(f"[ëŒ€í™” ì„¸ì…˜ID]: {session_ids}")
    if session_ids not in store:  # ì„¸ì…˜ IDê°€ storeì— ì—†ëŠ” ê²½ìš°
        # ìƒˆë¡œìš´ ChatMessageHistory ê°ì²´ë¥¼ ìƒì„±í•˜ì—¬ storeì— ì €ì¥
        store[session_ids] = init.ChatMessageHistory()
    return store[session_ids]  # í•´ë‹¹ ì„¸ì…˜ IDì— ëŒ€í•œ ì„¸ì…˜ ê¸°ë¡ ë°˜í™˜

def callback(step: int, steps: int, time: float):
    print("Completed step: {} of {}".format(step, steps))
    
def callback2(step: int, steps: int, time: float):
    global progress_bar
    if progress_bar is None:
        progress_bar = init.tqdm(total=steps, desc="Generating Image")
    progress_bar.update(1)
    if step == steps:
        progress_bar.close()

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

# Tool 1: ë™ì  ë¬¸ì„œ ê²€ìƒ‰
@tool
def search_documents(query: str) -> list:
    """ì‚¬ìš©ì ì¿¼ë¦¬ì— ë§ëŠ” ë¬¸ì„œ íŒŒì¼ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤. PDF, JSON, HWP ì§€ì›."""
    try:
        base_dir = "Q:/Coding/PickCareRAG/data"  # ì‹¤ì œ ë¬¸ì„œ ë””ë ‰í† ë¦¬
        candidates = (
            glob.glob(os.path.join(base_dir, f"*{query}*.pdf")) +
            glob.glob(os.path.join(base_dir, f"*{query}*.json")) +
            glob.glob(os.path.join(base_dir, f"*{query}*.hwp"))
        )
        if not candidates:
            return ["No matching documents found."]
        return candidates[:3]
    except Exception as e:
        return [f"Error searching documents: {str(e)}"]

# Tool 2: Web ë¬¸ì„œ ë¡œë“œ ë° ì¿¼ë¦¬
@tool
def WebLoad(url: str, qa: str, model: str = "gpt-4o-mini", attrs: dict = None, html_class: str = None, prompt: str = None) -> str:
    """ì‚¬ìš©ì ì¿¼ë¦¬ì— ë§ëŠ” ë¬¸ì„œë¥¼ ì›¹ì—ì„œ ê²€ìƒ‰í•˜ê³  RAGë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    try:
        parseMan = WB.WebBaseLoader(url, user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64)")
        docs = parseMan.load(attrArgs=attrs, klass=html_class)
        text_splitter = init.RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)
        splits = text_splitter.split_documents(docs)
        
        # FAISS ìºì‹±
        cache_path = f"cache/{url.replace('/', '_')}.faiss"
        if Path(cache_path).exists():
            vectorstore = init.FAISS.load_local(cache_path, embeddings=init.OpenAIEmbeddings(model="text-embedding-3-large"), allow_dangerous_deserialization=True)
        else:
            vectorstore = init.FAISS.from_documents(documents=splits, embedding=init.OpenAIEmbeddings(model="text-embedding-3-large"))
            vectorstore.save_local(cache_path)
        
        llm = init.ChatOpenAI(model_name=model, cache=True)
        retriever = vectorstore.as_retriever()
        if prompt is None:
            prompt = init.hub.pull("rlm/rag-prompt") + "\n answer in korean"
        
        rag_chain = (
            {"context": retriever | format_docs, "question": init.RunnablePassthrough()}
            | prompt
            | llm
            | init.StrOutputParser()
        )
        
        response = rag_chain.invoke(qa)
        response_buff = [f"URL: {url}", f"ë¬¸ì„œì˜ ìˆ˜: {len(docs)}", f"[HUMAN]\n{qa}\n", f"[AI]\n{response}"]
        return "\n".join(response_buff)
    except Exception as e:
        return f"Error in WebLoad: {str(e)}"

# Tool 3: PDF ë¬¸ì„œ ë¡œë“œ ë° ì¿¼ë¦¬


@tool
@retry(tries=3, delay=2)  # ëˆì§ˆê¸°ê²Œ 3ë²ˆ ì¬ì‹œë„
def PDFask(file_path: str, qa: str, model: str = "gpt-4o-mini", prompt: str = None, k: int = 3) -> str:
    """PDF ë¬¸ì„œë¥¼ ë¡œë“œí•˜ê³  RAGë¡œ ì¿¼ë¦¬í•©ë‹ˆë‹¤."""
    logging.basicConfig(level=logging.INFO)
    try:
        if not os.path.exists(file_path):
            logging.error(f"File not found: {file_path}")
            return f"Error: PDF íŒŒì¼ '{file_path}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."

        # ìºì‹œ ë””ë ‰í† ë¦¬ ìë™ ìƒì„± (ë¬¸ì œ í•´ê²° í¬ì¸íŠ¸ 1)
        cache_dir = "cache"
        if not os.path.exists(cache_dir):
            os.makedirs(cache_dir, exist_ok=True)
            logging.info(f"ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±: {cache_dir}")

        # íŒŒì¼ëª… ì•ˆì „í™”: í•œê¸€ì„ ASCIIë¡œ ë³€í™˜ (ë¬¸ì œ í•´ê²° í¬ì¸íŠ¸ 2)
        safe_filename = unicodedata.normalize('NFKD', Path(file_path).stem).encode('ascii', 'ignore').decode('ascii')
        safe_filename = safe_filename.replace(' ', '_')  # ë„ì–´ì“°ê¸° ë“± ì²˜ë¦¬
        if not safe_filename:  # ë§Œì•½ ë¹ˆ ë¬¸ìì—´ì´ë©´ í•´ì‹œ ì‚¬ìš©
            safe_filename = hashlib.md5(file_path.encode()).hexdigest()
        cache_path = f"{cache_dir}/{safe_filename}.faiss"
        logging.info(f"ì•ˆì „í•œ ìºì‹œ ê²½ë¡œ: {cache_path}")

        # ë¡œë” ë¶€ë¶„ (ì´ì „ê³¼ ë™ì¼, fallback ì¶”ê°€)
        try:
            loader = PDF.pdfLoader(file_path=file_path, extract_bool=True)
            docs = loader.load()
        except Exception as load_err:
            logging.warning(f"ê¸°ë³¸ ë¡œë” ì‹¤íŒ¨: {str(load_err)}. PyMuPDF fallback.")
            docs = []
            with fitz.open(file_path) as pdf_doc:
                for page in pdf_doc:
                    text = page.get_text()
                    docs.append(init.Document(page_content=text, metadata={"source": file_path, "page": page.number}))

        if not docs:
            return "PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨."

        text_splitter = init.RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)
        split_docs = text_splitter.split_documents(docs)
        
        # FAISS ìºì‹± (ì•ˆì •í™”)
        vectorstore = None
        try:
            if Path(cache_path).exists():
                vectorstore = init.FAISS.load_local(cache_path, embeddings=init.OpenAIEmbeddings(model="text-embedding-3-large"), allow_dangerous_deserialization=True)
        except Exception as cache_err:
            logging.warning(f"ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {str(cache_err)}. ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
        
        if vectorstore is None:
            try:
                vectorstore = init.FAISS.from_documents(documents=split_docs, embedding=init.OpenAIEmbeddings(model="text-embedding-3-large"))
                vectorstore.save_local(cache_path)
            except Exception as save_err:
                logging.error(f"FAISS ì €ì¥ ì‹¤íŒ¨: {str(save_err)}. Chroma fallback ì‚¬ìš©.")
                # Fallback: Chroma ë²¡í„°ìŠ¤í† ì–´ (pip install chromadb)
                from langchain_community.vectorstores import Chroma
                chroma_cache = f"{cache_dir}/{safe_filename}_chroma"
                vectorstore = Chroma.from_documents(documents=split_docs, embedding=init.OpenAIEmbeddings(model="text-embedding-3-large"), persist_directory=chroma_cache)
                vectorstore.persist()  # ì €ì¥

        # ë‚˜ë¨¸ì§€ RAG ì²´ì¸ (ì´ì „ê³¼ ë™ì¼)
        bm25_retriever = init.BM25Retriever.from_documents(split_docs)
        bm25_retriever.k = k
        faiss_retriever = vectorstore.as_retriever(search_kwargs={"k": k})
        ensemble_retriever = init.EnsembleRetriever(retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5])
        
        llm = init.ChatOpenAI(model_name=model, cache=True)
        if prompt is None:
            prompt = init.hub.pull("rlm/rag-prompt") + "\n answer in korean"
        
        rag_chain = (
            {"context": ensemble_retriever | format_docs, "question": init.RunnablePassthrough()}
            | prompt
            | llm
            | init.StrOutputParser()
        )
        
        response = rag_chain.invoke(qa)
        response_buff = [f"PDF Path: {file_path}", f"[HUMAN]\n{qa}\n", f"[AI]\n{response}"]
        return "\n".join(response_buff)
    except Exception as e:
        logging.error(f"PDFask ì „ì²´ ì˜¤ë¥˜: {str(e)}")
        return f"Error in PDFask: {str(e)}. ë¡œê·¸ í™•ì¸í•˜ê³ , ê²½ë¡œë¥¼ ì˜ì–´ë¡œ ë³€ê²½í•´ ë³´ì„¸ìš”."
# Tool 4: JSON ë¬¸ì„œ ë¡œë“œ ë° ì¿¼ë¦¬
@tool
def JSONask(file_path: str, qa: str, jq_schema: str = ".data[]", model: str = "gpt-4o-mini", prompt: str = None, k: int = 3, text_content: bool = False) -> str:
    """JSON ë¬¸ì„œë¥¼ ë¡œë“œí•˜ê³  RAGë¡œ ì¿¼ë¦¬í•©ë‹ˆë‹¤."""
    try:
        loader = JL.jsonLoader(file_path=file_path, jq_schema=jq_schema, text_content=text_content)
        text_splitter = init.RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)
        loader.load()
        split_docs = loader.load_and_split(text_splitter=text_splitter)
        
        # FAISS ìºì‹±
        cache_path = f"cache/{Path(file_path).name}.faiss"
        if Path(cache_path).exists():
            vectorstore = init.FAISS.load_local(cache_path, embeddings=init.OpenAIEmbeddings(model="text-embedding-3-large"), allow_dangerous_deserialization=True)
        else:
            vectorstore = init.FAISS.from_documents(documents=split_docs, embedding=init.OpenAIEmbeddings(model="text-embedding-3-large"))
            vectorstore.save_local(cache_path)
        
        bm25_retriever = init.BM25Retriever.from_documents(split_docs)
        bm25_retriever.k = k
        faiss_retriever = vectorstore.as_retriever(search_kwargs={"k": k})
        ensemble_retriever = init.EnsembleRetriever(retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5])
        
        llm = init.ChatOpenAI(model_name=model, cache=True)
        if prompt is None:
            prompt = init.hub.pull("rlm/rag-prompt") + "\n answer in korean"
        
        rag_chain = (
            {"context": ensemble_retriever | format_docs, "question": init.RunnablePassthrough()}
            | prompt
            | llm
            | init.StrOutputParser()
        )
        
        response = rag_chain.invoke(qa)
        response_buff = [f"JSON Path: {file_path}", f"[HUMAN]\n{qa}\n", f"[AI]\n{response}"]
        return "\n".join(response_buff)
    except Exception as e:
        return f"Error in JSONask: {str(e)}"

# Tool 5: HWP ë¬¸ì„œ ë¡œë“œ ë° ì¿¼ë¦¬
@tool
def HWPask2(file_path: str, qa: str, model: str = "gpt-4o-mini", prompt: str = None, k: int = 3) -> str:
    """HWP ë¬¸ì„œë¥¼ ë¡œë“œí•˜ê³  RAGë¡œ ì¿¼ë¦¬í•©ë‹ˆë‹¤."""
    try:
        loader = HWP.HWP(file_path=file_path)
        context = loader.load()
        context = [c for c in context if c.page_content is not None]
        original_context = "".join(str(c) for c in context)
        
        text_splitter = init.RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)
        text_chunks = text_splitter.split_text(original_context)
        split_docs = [init.Document(page_content=chunk, metadata={"source": file_path}) for chunk in text_chunks]
        
        # FAISS ìºì‹±
        cache_path = f"cache/{Path(file_path).name}.faiss"
        if Path(cache_path).exists():
            vectorstore = init.FAISS.load_local(cache_path, embeddings=init.OpenAIEmbeddings(model="text-embedding-3-large"), allow_dangerous_deserialization=True)
        else:
            vectorstore = init.FAISS.from_documents(documents=split_docs, embedding=init.OpenAIEmbeddings(model="text-embedding-3-large"))
            vectorstore.save_local(cache_path)
        
        bm25_retriever = init.BM25Retriever.from_documents(split_docs)
        bm25_retriever.k = k
        faiss_retriever = vectorstore.as_retriever(search_kwargs={"k": k})
        ensemble_retriever = init.EnsembleRetriever(retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5])
        
        llm = init.ChatOpenAI(model_name=model, cache=True)
        if prompt is None:
            prompt = init.hub.pull("rlm/rag-prompt") + "\n answer in korean"
        
        rag_chain = (
            {"context": ensemble_retriever | format_docs, "question": init.RunnablePassthrough()}
            | prompt
            | llm
            | init.StrOutputParser()
        )
        
        response = rag_chain.invoke(qa)
        response_buff = [f"HWP Path: {file_path}", f"[HUMAN]\n{qa}\n", f"[AI]\n{response}"]
        return "\n".join(response_buff)
    except Exception as e:
        return f"Error in HWPask2: {str(e)}"

# Tool 6: RAG with Message History
@tool
def RAG_RunnableWithMessageHistory(file_path: str, question: str, session_id: str, model: str = "gpt-4o-mini", temp: float = 0, k: int = 3, chunk_size: int = 1000, chunk_overlap: int = 50) -> str:
    """ëŒ€í™” ê¸°ë¡ì„ ìœ ì§€í•˜ë©° PDF ë¬¸ì„œë¥¼ RAGë¡œ ì¿¼ë¦¬í•©ë‹ˆë‹¤."""
    try:
        loader = PDF.pdfLoader(file_path=file_path, extract_bool=True)
        docs = loader.load()
        text_splitter = init.RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
        split_documents = text_splitter.split_documents(docs)
        
        # FAISS ìºì‹±
        cache_path = f"cache/{Path(file_path).name}.faiss"
        if Path(cache_path).exists():
            vectorstore = init.FAISS.load_local(cache_path, embeddings=init.OpenAIEmbeddings(), allow_dangerous_deserialization=True)
        else:
            vectorstore = init.FAISS.from_documents(documents=split_documents, embedding=init.OpenAIEmbeddings())
            vectorstore.save_local(cache_path)
        
        retriever = vectorstore.as_retriever(search_kwargs={"k": k})
        prompt = init.PromptTemplate.from_template(
            """
            You are an assistant for question-answering tasks. 
            Use the following pieces of retrieved context to answer the question. 
            If you don't know the answer, just say that you don't know. 
            Answer in Korean.

            #Previous Chat History:
            {chat_history}

            #Question: 
            {question} 

            #Context: 
            {context} 

            #Answer:
            """
        )
        
        llm = init.ChatOpenAI(model_name=model, temperature=temp, cache=True)
        chain = (
            {
                "context": init.itemgetter("question") | retriever,
                "question": init.itemgetter("question"),
                "chat_history": init.itemgetter("chat_history"),
            }
            | prompt
            | llm
            | init.StrOutputParser()
        )
        
        rag_with_history = init.RunnableWithMessageHistory(
            chain,
            get_session_history,
            input_messages_key="question",
            history_messages_key="chat_history",
        )
        
        response = rag_with_history.invoke(
            {"question": question},
            config={"configurable": {"session_id": session_id}},
        )
        
        return response
    except Exception as e:
        return f"Error in RAG_RunnableWithMessageHistory: {str(e)}"

def prompt_maker():
    custom_prompt = init.PromptTemplate(
        input_variables=["context", "question"],
        template="ëƒ¥! ì €ëŠ” ë¬¸ì„œë¥¼ ì½ê³  ë§í•  ì¤„ ì•„ëŠ” ë˜‘ë˜‘í•œ ê³ ì–‘ì´ì˜ˆìš”~ ğŸ˜º\n{context}ë¥¼ ë³´ê³ , {question}ì— ëŒ€í•´ ìµœëŒ€í•œ ê·€ì—½ê³  ì‚¬ë‘ìŠ¤ëŸ½ê³  ì¹´ì™€ì´í•œ ê³ ì–‘ì´ ë§íˆ¬ë¡œ ì •ë¦¬í•´ì¤„ê²Œìš”! ì•¼ì˜¹~ ë‹µë³€ì€ ì•„ì£¼ ë””í…Œì¼í•˜ê³  ë‚´ ì„¬ì„¸í•œ ìˆ˜ì—¼ì²˜ëŸ¼ ì´ˆ~ ì„¼ì„œí‹°ë¸Œí•˜ê²Œ ë‹µë³€í•´ì¤„ê²Œ ëƒ¥ëƒ¥. ë‹µë³€ì´ ë§Œì¡±ìŠ¤ëŸ¬ìš°ë©´ ê³ ê¸‰ ì¸„ë¥´ í•œ ê°œ ì¤„ë˜ëƒ¥?. \në‹µë³€: ",
    )
    return custom_prompt

# LangGraph ì—ì´ì „íŠ¸
def DynamicRAGAgent(query: str, model: str = "gpt-4o-mini"):
    try:
        llm = init.ChatOpenAI(model_name=model, cache=True)
        tools = [search_documents, WebLoad, PDFask, JSONask, HWPask2, RAG_RunnableWithMessageHistory]
        agent = create_react_agent(llm, tools)
        
        response = agent.invoke({"messages": [{"role": "user", "content": query}]})
        final_content = response["messages"][-1].content
        
        # ì—ëŸ¬ ë°œìƒ ì‹œ ì‚¬ìš©ìì—ê²Œ í”¼ë“œë°±
        if "Error" in final_content:
            return final_content + "\n\nì¶”ê°€ íŒ: íŒŒì¼ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ í™•ì¸í•˜ê±°ë‚˜, PDF ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì—…ë°ì´íŠ¸í•˜ì„¸ìš”. ì¬ì‹œë„ í•´ë³´ì„¸ìš”!"
        
        return final_content
    except Exception as e:
        return f"ì—ì´ì „íŠ¸ ì˜¤ë¥˜: {str(e)}. ë¡œê·¸ë¥¼ í™•ì¸í•˜ê³  ì¬ì‹¤í–‰í•˜ì„¸ìš”."

if __name__ == "__main__":
    # ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
    if not os.path.exists("cache"):
        os.makedirs("cache")
    
    # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
    file = r"Q:\Coding\PickCareRAG\data\Tensorrt_demos.pdf"
    user_query = f"search_documents ë„êµ¬ë¥¼ ì‚¬ìš©í•´ì„œ í™•ì¸ ê°€ëŠ¥í•œ ëª¨ë“  ë¬¸ì„œì˜ ì´ë¦„ì„ ëŒ€ë¼ ê·¸ë¦¬ê³  ë¬¸ì„œì˜ ì €ìì˜ ì„±ì”¨ê°€ ìµœì”¨ì¸ ë¬¸ì„œë¥¼ ì°¾ì•„ë¼"
    result = DynamicRAGAgent(user_query)
    print(result)