import init
import WebBaseLoader as WB
import pdfLoader as PDF
import jsonLoader as JL
import pptLoader as PL
import csvLoader as CL
import CLIP_RAG as CLIP
import testClass as TC
if init.platform.system() == "Windows": 
    import hwpLoader as HL
import docxLoader_old as WL
from test_grok import create_image
from trellis import create_3d_from_image
#import HWP
import RAG
from dotenv import load_dotenv
from langchain_teddynote import logging
import HWP_async as HWP
load_dotenv()

logging.langsmith("Main Test")

def format_docs(docs):
    # ê²€ìƒ‰í•œ ë¬¸ì„œ ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ ë¬¸ë‹¨ìœ¼ë¡œ í•©ì³ì¤ë‹ˆë‹¤.
    return "\n\n".join(doc.page_content for doc in docs)


def prompt_maker():
    custom_prompt = init.PromptTemplate(
    input_variables=["context", "question"],
    template="ëƒ¥! ì €ëŠ” ë¬¸ì„œë¥¼ ì½ê³  ë§í•  ì¤„ ì•„ëŠ” ë˜‘ë˜‘í•œ ê³ ì–‘ì´ì˜ˆìš”~ ğŸ˜º\n{context}ë¥¼ ë³´ê³ , {question}ì— ëŒ€í•´ ìµœëŒ€í•œ ê·€ì—½ê³  ì‚¬ë‘ìŠ¤ëŸ½ê³  ì¹´ì™€ì´í•œ ê³ ì–‘ì´ ë§íˆ¬ë¡œ ì •ë¦¬í•´ì¤„ê²Œìš”! ì•¼ì˜¹~ ë‹µë³€ì€ ì•„ì£¼ ë””í…Œì¼í•˜ê³  ë‚´ ì„¬ì„¸í•œ ìˆ˜ì—¼ì²˜ëŸ¼ ì´ˆ~ ì„¼ì„œí‹°ë¸Œí•˜ê²Œ ë‹µë³€í•´ì¤„ê²Œ ëƒ¥ëƒ¥. ë‹µë³€ì´ ë§Œì¡±ìŠ¤ëŸ¬ìš°ë©´ ê³ ê¸‰ ì¸„ë¥´ í•œ ê°œ ì¤„ë˜ëƒ¥?. \në‹µë³€: ",
    )
    return custom_prompt


def HWPask2(file_path, QA, model = "gpt-5", prompt = None, k = 3):
    loader = HWP.HWP(file_path= file_path)
    context = loader.load()
    idx = 0
    for c in context:
        if c.page_content == None:
            context.pop(idx)
    original_context = ""
    for c in context:
        original_context += str(c)
    
    context = original_context
            

    text_splitter = init.RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 50)
    text_chunks = text_splitter.split_text(context)
    # ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ë¥¼ Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    split_docs = [init.Document(page_content=chunk, metadata={"source": file_path}) for chunk in text_chunks]
    #vectorstore = init.FAISS.from_documents(documents = split_docs, embedding = init.OpenAIEmbeddings(model = "text-embedding-3-large"))
    
    bm25_retriever = init.BM25Retriever.from_documents(split_docs)    
    bm25_retriever.k = k
    faiss_vectorstore = init.FAISS.from_documents(split_docs, init.OpenAIEmbeddings(model="text-embedding-3-large"))
    faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs = {"k" : k})
    #ì•™ìƒë¸” ë¦¬íŠ¸ë¦¬ë²„ë¥¼ ì´ˆê¸°í™”
    ensemble_retiever = init.EnsembleRetriever(
        retrievers=[bm25_retriever, faiss_retriever], weight = [0.5, 0.5]
    )
    
    if prompt == None:
        prompt = init.hub.pull("rlm/rag-prompt") #í”„ë¡¬í”„íŠ¸
    
    prompt += "\n answer in korean"
    
    
    llm = init.ChatOpenAI(model_name = model)
    
    rag_chain = (
        {"context" : ensemble_retiever | format_docs, "question" : init.RunnablePassthrough()}
        #{"context" : init.RunnableLambda(format_docs) | ensemble_retiever, "question" : init.RunnablePassthrough()}
        |prompt
        |llm
        |init.StrOutputParser()
    )
    
    response = rag_chain.invoke(QA)
    response_buff = list()
    response_buff.append(f"HWP Path: {file_path}")
    response_buff.append(f"[HUMAN]\n{QA}\n")
    response_buff.append(f"[AI]\n{response}")
    return response_buff




if __name__ == "__main__":
    # TC.TestClass.test_webBase()
    # TC.TestClass.test_webBase2()
    # TC.TestClass.testJSON()
    # TC.TestClass.testPDF()
    # TC.TestClass.testPPT()
    # loader = CL.csvLoader(file_path = "data/titanic.csv")
    # docs = loader.load()
    # for elem in docs:
    #     print(elem.page_content)
    # QA = "ì‚¼ì„± ê°€ìš°ìŠ¤ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”"
    # file = "data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf"
    # file4 = r"Tensorrt_demos.pdf"
    # file3 = "data/people.json"
    # pdfQuery = RAG.PDFask(model = "gpt-5-mini", QA = QA, file_path = file)
    # for elem in pdfQuery:
    #     print(elem)
    # global store
    # store = {}
    # session_id = {'session_id' : 'rag123'}
    # ask = {'system': 'ë‹¹ì‹ ì€ Question-Answering ì±—ë´‡ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.', 'question': 'ì£¼ì–´ì§„ ìë£Œì—ì„œ í•µì‹¬ ì‚¬í•­ì„ ìš”ì•½í•´ì„œ ë…¸ë˜ë¡œ ë§Œë“¤ì–´ ì£¼ì„¸ìš”'}
    # response = RAG.simpleChatWithHistory(ask)
    # response = RAG.RAG_RunnableWithMessageHistory(file_path = file, ask = ask, session_id= session_id)
    # print(response)
    # import unstructured.partition.pdf
    # print("ì„¤ì¹˜ ì„±ê³µ!")
    
    # docx_loader = WL.docxLoader(file_path="data/sample-word-document.docx")
    # docx_doc = docx_loader.load()
    # print("í™•ì¸")
    # print(docx_doc)
    
    
    # clip = CLIP.CLIP(4000, 0, rf"{file4}", fpath = "data/")
    # chain = clip.load()
    # query = "ë‹¤ìŒ ë¬¸ì„œê°€ ë§í•˜ê³ ìí•˜ëŠ” ë‚´ìš©ì— ëŒ€í•´ì„œ ì‰¬ìš´ ìš©ì–´ë¡œ ì„¤ëª…í•˜ì„¸ìš”.  ê·¸ë¦¬ê³  ìƒì„¸íˆ ë‚´ìš©ì„ ë¶„ì„í•˜ì„¸ìš”. ê·¸ë¦¬ê³  ì£¼ì–´ì§„ ì´ë¯¸ì§€ë“¤ì— ëŒ€í•´ì„œ ì„¤ëª…í•˜ì„¸ìš”. ê·¸ë¦¬ê³  ë‹¹ì‹ ì´ ì •ë¦¬í•œ ëª¨ë“  ë‚´ìš©ì„ ì¼ë³¸ì–´ë¡œ ë²ˆì—­í•˜ì„¸ìš”"
    # response = chain.invoke(query, limit = 6)
    # print(response)
    # test_json = JL.jsonLoader(file_path = file3, jq_schema=".[].phoneNumbers", text_content= False)
    # text_splitter = init.RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 50)
    # print(test_json.load())
    
    # json_response = RAG.JSONask(file_path = file3, jq_schema= ".[].phoneNumbers", QA = "ë‹¤ìŒ ë¬¸ì„œë¥¼ ë¶„ì„í•˜ì—¬ ìš”ì•½í•˜ì„¸ìš”")
    # print(json_response)
    # custom_prompt = RAG.prompt_maker()
    # if init.platform.system() == "Windows": 
    #     hwp = HL.hwpLoader(r"Q:\Coding\PickCareRAG\ë””ì§€í„¸ ì •ë¶€í˜ì‹  ì¶”ì§„ê³„íš.hwp")
    #     docs = hwp.load2()
    #     print(docs)
        
    #     hwp_response = RAG.HWPask(file_path= r"Q:\Coding\PickCareRAG\ë””ì§€í„¸ ì •ë¶€í˜ì‹  ì¶”ì§„ê³„íš.hwp",prompt = custom_prompt,  QA = "í•´ë‹¹ ë¬¸ì„œë¥¼ ê³µë¶€í•˜ë¼ê³  ì”ì†Œë¦¬ë§Œ í•´ëŒ€ì„œ ì§‘ì„ ë›°ì³ë‚˜ê°€ì„œ ë°©ë‘í•˜ëŠ” í•œì°¸ ì§ˆí’ë…¸ë„ì˜ ì‹œê¸° ì†ì— ë†“ì—¬ì§„ ë°•í…Œë¦¬ì•„ë„ ì´í•´í•  ìˆ˜ ìˆì„ ë§Œí¼ ì‰½ê²Œ ì •ë¦¬í•˜ì„¸ìš”")
    #     for elem in hwp_response:
    #         print(elem, end = "", flush = True)
    
    # prompt = init.hub.pull("rlm/rag-prompt")
    # print(prompt)
    # if init.platform.system() == "Windows": 
    #     print("ìœˆë„ìš°")
    #     pdf_response = RAG.PDFask(file_path=r"Q:\Coding\PickCareRAG\data\Tensorrt_demos.pdf", model = "gpt-5",  QA = "í•´ë‹¹ ë¬¸ì„œë¥¼ ê¸¸ê°€ë˜ ì°¸ìƒˆë„ ì´í•´í•  ìˆ˜ ìˆì„ ë§Œí¼ ì‰½ê²Œ ì •ë¦¬í•˜ì„¸ìš”", prompt=custom_prompt)
    # elif init.platform.system() == "Linux":
    #     print("ë¦¬ëˆ…ìŠ¤")
    #     pdf_response = RAG.PDFask(file_path=r"/mnt/q/Coding/PickCareRAG/data/Tensorrt_demos.pdf", model = "gpt-5",  QA = "í•´ë‹¹ ë¬¸ì„œë¥¼ ì¶©ì¹˜ì— ê±¸ë¦° ë‹¤ëŒì¥ë„ ì´í•´í•  ìˆ˜ ìˆì„ ë§Œí¼ ì‰½ê²Œ ì •ë¦¬í•˜ì„¸ìš”", prompt=custom_prompt)
    # for elem in pdf_response:
    #     print(elem, end = "", flush = True)
    
    # RAG.create_diffusion_image("cute_cat", "very very cute, lovely, precious kitty staring at me")
        
    
    # #create_image(prompt = "A photo of a cute kitten in Kawhi asking for a snack. An animation from the 1980s in Japan", file_name = "output_images/lovely_cat.png")
    
    # #create_3d_from_image(file_path= "data/doggum.jpg", output_file = "cutecat.mp4")
    
    # hwp_response = RAG.HWPask2(file_path= r"Q:\Coding\PickCareRAG\ë””ì§€í„¸ ì •ë¶€í˜ì‹  ì¶”ì§„ê³„íš.hwp",prompt = custom_prompt,  QA = "í•´ë‹¹ ë¬¸ì„œë¥¼ ê³µë¶€í•˜ë¼ê³  ì”ì†Œë¦¬ë§Œ í•´ëŒ€ì„œ ì§‘ì„ ë›°ì³ë‚˜ê°€ì„œ ë°©ë‘í•˜ëŠ” í•œì°¸ ì§ˆí’ë…¸ë„ì˜ ì‹œê¸° ì†ì— ë†“ì—¬ì§„ ë°•í…Œë¦¬ì•„ë„ ì´í•´í•  ìˆ˜ ìˆì„ ë§Œí¼ ì‰½ê²Œ ì •ë¦¬í•˜ì„¸ìš”")
    # for elem in hwp_response:
    #     print(elem)
    
    # QA = "ì‚¼ì„± ê°€ìš°ìŠ¤ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”"
    # file = "data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf"
    # pdfQuery = RAG.PDFask(model = "gpt-5-mini", QA = QA, file_path = file)
    # res = ""
    # for elem in pdfQuery:
    #     res += elem
        
    # print(res)
    
    hwp_response = RAG.HWPask2(file_path= r"Q:\Coding\PickCareRAG\ë””ì§€í„¸ ì •ë¶€í˜ì‹  ì¶”ì§„ê³„íš.hwp",prompt = prompt_maker(),  QA = "í•´ë‹¹ ë¬¸ì„œë¥¼ ê³µë¶€í•˜ë¼ê³  ì”ì†Œë¦¬ë§Œ í•´ëŒ€ì„œ ì§‘ì„ ë›°ì³ë‚˜ê°€ì„œ ë°©ë‘í•˜ëŠ” í•œì°¸ ì§ˆí’ë…¸ë„ì˜ ì‹œê¸° ì†ì— ë†“ì—¬ì§„ ë°•í…Œë¦¬ì•„ë„ ì´í•´í•  ìˆ˜ ìˆì„ ë§Œí¼ ì‰½ê²Œ ì •ë¦¬í•˜ì„¸ìš”")
    for elem in hwp_response:
        print(elem, end = "", flush = True)
    
        
    